delta_t 0.000010 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.00999983
0.0199987
0.0299955
0.0399893
0.0499792
0.059964
0.0699428
0.0799147
0.0898785
0.0998334
Process [1]
0.109778
0.119712
0.129634
0.139543
0.149438
0.159318
0.169182
0.17903
0.188859
0.198669
Process [2]
0.20846
0.21823
0.227978
0.237703
0.247404
0.257081
0.266731
0.276356
0.285952
0.29552
Process [3]
0.305059
0.314567
0.324043
0.333487
0.342898
0.352274
0.361615
0.37092
0.380188
0.389418
Process [4]
0.398609
0.40776
0.416871
0.425939
0.434966
0.443948
0.452886
0.461779
0.470626
0.479426
Process [5]
0.488177
0.49688
0.505533
0.514136
0.522687
0.531186
0.539632
0.548024
0.556361
0.564642
Process [6]
0.572867
0.581035
0.589145
0.597195
0.605186
0.613117
0.620986
0.628793
0.636537
0.644218
Process [7]
0.651834
0.659385
0.66687
0.674288
0.681639
0.688921
0.696135
0.703279
0.710353
0.717356
Process [8]
0.724287
0.731146
0.737931
0.744643
0.75128
0.757843
0.764329
0.770739
0.777072
0.783327
Process [9]
0.789504
0.795602
0.80162
0.807558
0.813416
0.819192
0.824886
0.830497
0.836026
0.
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [1]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [2]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [3]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [4]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [5]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [6]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [7]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [8]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Process [9]
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
Mat Object: 10 MPI processes
  type: mpiaij
row 0: (0, 0.8)  (1, 0.1) 
row 1: (0, 0.1)  (1, 0.8)  (2, 0.1) 
row 2: (1, 0.1)  (2, 0.8)  (3, 0.1) 
row 3: (2, 0.1)  (3, 0.8)  (4, 0.1) 
row 4: (3, 0.1)  (4, 0.8)  (5, 0.1) 
row 5: (4, 0.1)  (5, 0.8)  (6, 0.1) 
row 6: (5, 0.1)  (6, 0.8)  (7, 0.1) 
row 7: (6, 0.1)  (7, 0.8)  (8, 0.1) 
row 8: (7, 0.1)  (8, 0.8)  (9, 0.1) 
row 9: (8, 0.1)  (9, 0.8)  (10, 0.1) 
row 10: (9, 0.1)  (10, 0.8)  (11, 0.1) 
row 11: (10, 0.1)  (11, 0.8)  (12, 0.1) 
row 12: (11, 0.1)  (12, 0.8)  (13, 0.1) 
row 13: (12, 0.1)  (13, 0.8)  (14, 0.1) 
row 14: (13, 0.1)  (14, 0.8)  (15, 0.1) 
row 15: (14, 0.1)  (15, 0.8)  (16, 0.1) 
row 16: (15, 0.1)  (16, 0.8)  (17, 0.1) 
row 17: (16, 0.1)  (17, 0.8)  (18, 0.1) 
row 18: (17, 0.1)  (18, 0.8)  (19, 0.1) 
row 19: (18, 0.1)  (19, 0.8)  (20, 0.1) 
row 20: (19, 0.1)  (20, 0.8)  (21, 0.1) 
row 21: (20, 0.1)  (21, 0.8)  (22, 0.1) 
row 22: (21, 0.1)  (22, 0.8)  (23, 0.1) 
row 23: (22, 0.1)  (23, 0.8)  (24, 0.1) 
row 24: (23, 0.1)  (24, 0.8)  (25, 0.1) 
row 25: (24, 0.1)  (25, 0.8)  (26, 0.1) 
row 26: (25, 0.1)  (26, 0.8)  (27, 0.1) 
row 27: (26, 0.1)  (27, 0.8)  (28, 0.1) 
row 28: (27, 0.1)  (28, 0.8)  (29, 0.1) 
row 29: (28, 0.1)  (29, 0.8)  (30, 0.1) 
row 30: (29, 0.1)  (30, 0.8)  (31, 0.1) 
row 31: (30, 0.1)  (31, 0.8)  (32, 0.1) 
row 32: (31, 0.1)  (32, 0.8)  (33, 0.1) 
row 33: (32, 0.1)  (33, 0.8)  (34, 0.1) 
row 34: (33, 0.1)  (34, 0.8)  (35, 0.1) 
row 35: (34, 0.1)  (35, 0.8)  (36, 0.1) 
row 36: (35, 0.1)  (36, 0.8)  (37, 0.1) 
row 37: (36, 0.1)  (37, 0.8)  (38, 0.1) 
row 38: (37, 0.1)  (38, 0.8)  (39, 0.1) 
row 39: (38, 0.1)  (39, 0.8)  (40, 0.1) 
row 40: (39, 0.1)  (40, 0.8)  (41, 0.1) 
row 41: (40, 0.1)  (41, 0.8)  (42, 0.1) 
row 42: (41, 0.1)  (42, 0.8)  (43, 0.1) 
row 43: (42, 0.1)  (43, 0.8)  (44, 0.1) 
row 44: (43, 0.1)  (44, 0.8)  (45, 0.1) 
row 45: (44, 0.1)  (45, 0.8)  (46, 0.1) 
row 46: (45, 0.1)  (46, 0.8)  (47, 0.1) 
row 47: (46, 0.1)  (47, 0.8)  (48, 0.1) 
row 48: (47, 0.1)  (48, 0.8)  (49, 0.1) 
row 49: (48, 0.1)  (49, 0.8)  (50, 0.1) 
row 50: (49, 0.1)  (50, 0.8)  (51, 0.1) 
row 51: (50, 0.1)  (51, 0.8)  (52, 0.1) 
row 52: (51, 0.1)  (52, 0.8)  (53, 0.1) 
row 53: (52, 0.1)  (53, 0.8)  (54, 0.1) 
row 54: (53, 0.1)  (54, 0.8)  (55, 0.1) 
row 55: (54, 0.1)  (55, 0.8)  (56, 0.1) 
row 56: (55, 0.1)  (56, 0.8)  (57, 0.1) 
row 57: (56, 0.1)  (57, 0.8)  (58, 0.1) 
row 58: (57, 0.1)  (58, 0.8)  (59, 0.1) 
row 59: (58, 0.1)  (59, 0.8)  (60, 0.1) 
row 60: (59, 0.1)  (60, 0.8)  (61, 0.1) 
row 61: (60, 0.1)  (61, 0.8)  (62, 0.1) 
row 62: (61, 0.1)  (62, 0.8)  (63, 0.1) 
row 63: (62, 0.1)  (63, 0.8)  (64, 0.1) 
row 64: (63, 0.1)  (64, 0.8)  (65, 0.1) 
row 65: (64, 0.1)  (65, 0.8)  (66, 0.1) 
row 66: (65, 0.1)  (66, 0.8)  (67, 0.1) 
row 67: (66, 0.1)  (67, 0.8)  (68, 0.1) 
row 68: (67, 0.1)  (68, 0.8)  (69, 0.1) 
row 69: (68, 0.1)  (69, 0.8)  (70, 0.1) 
row 70: (69, 0.1)  (70, 0.8)  (71, 0.1) 
row 71: (70, 0.1)  (71, 0.8)  (72, 0.1) 
row 72: (71, 0.1)  (72, 0.8)  (73, 0.1) 
row 73: (72, 0.1)  (73, 0.8)  (74, 0.1) 
row 74: (73, 0.1)  (74, 0.8)  (75, 0.1) 
row 75: (74, 0.1)  (75, 0.8)  (76, 0.1) 
row 76: (75, 0.1)  (76, 0.8)  (77, 0.1) 
row 77: (76, 0.1)  (77, 0.8)  (78, 0.1) 
row 78: (77, 0.1)  (78, 0.8)  (79, 0.1) 
row 79: (78, 0.1)  (79, 0.8)  (80, 0.1) 
row 80: (79, 0.1)  (80, 0.8)  (81, 0.1) 
row 81: (80, 0.1)  (81, 0.8)  (82, 0.1) 
row 82: (81, 0.1)  (82, 0.8)  (83, 0.1) 
row 83: (82, 0.1)  (83, 0.8)  (84, 0.1) 
row 84: (83, 0.1)  (84, 0.8)  (85, 0.1) 
row 85: (84, 0.1)  (85, 0.8)  (86, 0.1) 
row 86: (85, 0.1)  (86, 0.8)  (87, 0.1) 
row 87: (86, 0.1)  (87, 0.8)  (88, 0.1) 
row 88: (87, 0.1)  (88, 0.8)  (89, 0.1) 
row 89: (88, 0.1)  (89, 0.8)  (90, 0.1) 
row 90: (89, 0.1)  (90, 0.8)  (91, 0.1) 
row 91: (90, 0.1)  (91, 0.8)  (92, 0.1) 
row 92: (91, 0.1)  (92, 0.8)  (93, 0.1) 
row 93: (92, 0.1)  (93, 0.8)  (94, 0.1) 
row 94: (93, 0.1)  (94, 0.8)  (95, 0.1) 
row 95: (94, 0.1)  (95, 0.8)  (96, 0.1) 
row 96: (95, 0.1)  (96, 0.8)  (97, 0.1) 
row 97: (96, 0.1)  (97, 0.8)  (98, 0.1) 
row 98: (97, 0.1)  (98, 0.8)  (99, 0.1) 
row 99: (98, 0.1)  (99, 0.8)  (100, 0.1) 
row 100: (99, 0.1)  (100, 0.8) 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00240069
-0.00480607
-0.00722082
-0.00964963
-0.0120972
-0.0145681
-0.0170672
-0.0195989
-0.022168
-0.0247791
Process [1]
-0.0274368
-0.0301457
-0.0329103
-0.0357352
-0.038625
-0.041584
-0.0446168
-0.0477277
-0.0509212
-0.0542016
Process [2]
-0.0575733
-0.0610403
-0.064607
-0.0682775
-0.0720558
-0.075946
-0.0799521
-0.0840777
-0.0883269
-0.0927033
Process [3]
-0.0972104
-0.101852
-0.106631
-0.111551
-0.116616
-0.121828
-0.12719
-0.132705
-0.138376
-0.144206
Process [4]
-0.150197
-0.15635
-0.162669
-0.169155
-0.17581
-0.182635
-0.189633
-0.196803
-0.204148
-0.211668
Process [5]
-0.219363
-0.227234
-0.235282
-0.243506
-0.251906
-0.260481
-0.269231
-0.278156
-0.287252
-0.29652
Process [6]
-0.305958
-0.315563
-0.325334
-0.335268
-0.345363
-0.355614
-0.366021
-0.376578
-0.387282
-0.398129
Process [7]
-0.409116
-0.420237
-0.431488
-0.442864
-0.45436
-0.46597
-0.47769
-0.489512
-0.501431
-0.51344
Process [8]
-0.525534
-0.537705
-0.549947
-0.562251
-0.574612
-0.587022
-0.599473
-0.611957
-0.624467
-0.636995
Process [9]
-0.649531
-0.662069
-0.674599
-0.687114
-0.699605
-0.712063
-0.72448
-0.736846
-0.749154
0.
Norm of error 3.52005 ,The iteration is 10000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00559268
-0.0111871
-0.0167849
-0.0223879
-0.0279978
-0.0336163
-0.039245
-0.0448856
-0.0505398
-0.0562092
Process [1]
-0.0618955
-0.0676002
-0.0733249
-0.0790712
-0.0848405
-0.0906344
-0.0964542
-0.102301
-0.108177
-0.114083
Process [2]
-0.120021
-0.125991
-0.131995
-0.138034
-0.144109
-0.150221
-0.156371
-0.16256
-0.16879
-0.17506
Process [3]
-0.181371
-0.187725
-0.194121
-0.200561
-0.207045
-0.213574
-0.220147
-0.226765
-0.233429
-0.240138
Process [4]
-0.246893
-0.253693
-0.260539
-0.267431
-0.274367
-0.281349
-0.288375
-0.295446
-0.30256
-0.309717
Process [5]
-0.316917
-0.324158
-0.331441
-0.338763
-0.346125
-0.353524
-0.360961
-0.368433
-0.375939
-0.383479
Process [6]
-0.391051
-0.398653
-0.406283
-0.413941
-0.421624
-0.42933
-0.437058
-0.444806
-0.452572
-0.460354
Process [7]
-0.46815
-0.475957
-0.483773
-0.491597
-0.499425
-0.507256
-0.515087
-0.522915
-0.530738
-0.538554
Process [8]
-0.546359
-0.554151
-0.561927
-0.569685
-0.577422
-0.585134
-0.59282
-0.600476
-0.608099
-0.615686
Process [9]
-0.623235
-0.630742
-0.638204
-0.645619
-0.652983
-0.660294
-0.667548
-0.674742
-0.681873
0.
Norm of error 3.78693 ,The iteration is 20000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00643877
-0.0128778
-0.0193172
-0.0257572
-0.0321981
-0.0386401
-0.0450833
-0.051528
-0.0579744
-0.0644226
Process [1]
-0.0708727
-0.0773251
-0.0837798
-0.090237
-0.0966968
-0.103159
-0.109625
-0.116093
-0.122565
-0.129039
Process [2]
-0.135517
-0.141998
-0.148482
-0.15497
-0.16146
-0.167954
-0.174451
-0.180951
-0.187454
-0.19396
Process [3]
-0.200469
-0.20698
-0.213494
-0.22001
-0.226528
-0.233048
-0.239569
-0.246091
-0.252614
-0.259137
Process [4]
-0.265661
-0.272184
-0.278707
-0.285229
-0.291749
-0.298266
-0.304781
-0.311293
-0.317801
-0.324305
Process [5]
-0.330803
-0.337296
-0.343782
-0.350262
-0.356733
-0.363196
-0.369649
-0.376092
-0.382524
-0.388944
Process [6]
-0.395351
-0.401745
-0.408124
-0.414487
-0.420833
-0.427161
-0.433471
-0.439762
-0.446031
-0.452278
Process [7]
-0.458502
-0.464702
-0.470877
-0.477025
-0.483145
-0.489237
-0.495298
-0.501328
-0.507325
-0.513288
Process [8]
-0.519216
-0.525107
-0.530961
-0.536775
-0.542549
-0.548281
-0.55397
-0.559615
-0.565213
-0.570765
Process [9]
-0.576267
-0.58172
-0.587121
-0.59247
-0.597764
-0.603003
-0.608186
-0.613309
-0.618374
0.
Norm of error 3.6682 ,The iteration is 30000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00634177
-0.0126832
-0.0190241
-0.0253639
-0.0317026
-0.0380396
-0.0443748
-0.0507077
-0.0570382
-0.0633657
Process [1]
-0.0696901
-0.076011
-0.082328
-0.0886408
-0.0949491
-0.101253
-0.107551
-0.113844
-0.12013
-0.126411
Process [2]
-0.132685
-0.138951
-0.145211
-0.151462
-0.157706
-0.163941
-0.170167
-0.176384
-0.18259
-0.188787
Process [3]
-0.194973
-0.201148
-0.207311
-0.213463
-0.219602
-0.225728
-0.231841
-0.23794
-0.244025
-0.250095
Process [4]
-0.25615
-0.262188
-0.268211
-0.274216
-0.280205
-0.286175
-0.292127
-0.298059
-0.303973
-0.309865
Process [5]
-0.315738
-0.321588
-0.327417
-0.333223
-0.339006
-0.344765
-0.3505
-0.35621
-0.361894
-0.367551
Process [6]
-0.373182
-0.378784
-0.384359
-0.389904
-0.395419
-0.400904
-0.406358
-0.41178
-0.41717
-0.422526
Process [7]
-0.427848
-0.433135
-0.438387
-0.443603
-0.448782
-0.453923
-0.459026
-0.46409
-0.469114
-0.474097
Process [8]
-0.479039
-0.483938
-0.488795
-0.493608
-0.498376
-0.503099
-0.507777
-0.512407
-0.51699
-0.521525
Process [9]
-0.526011
-0.530447
-0.534833
-0.539167
-0.543449
-0.547679
-0.551855
-0.555977
-0.560044
0.
Norm of error 3.4118 ,The iteration is 40000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00593053
-0.0118606
-0.0177897
-0.0237174
-0.0296432
-0.0355666
-0.0414872
-0.0474045
-0.0533179
-0.0592271
Process [1]
-0.0651316
-0.0710308
-0.0769243
-0.0828117
-0.0886924
-0.0945659
-0.100432
-0.10629
-0.112139
-0.117979
Process [2]
-0.12381
-0.12963
-0.13544
-0.141239
-0.147026
-0.152802
-0.158565
-0.164315
-0.170051
-0.175773
Process [3]
-0.181481
-0.187174
-0.192852
-0.198513
-0.204158
-0.209786
-0.215396
-0.220988
-0.226562
-0.232117
Process [4]
-0.237652
-0.243166
-0.248661
-0.254134
-0.259585
-0.265015
-0.270421
-0.275804
-0.281164
-0.286499
Process [5]
-0.291809
-0.297094
-0.302353
-0.307586
-0.312791
-0.317969
-0.323119
-0.328241
-0.333333
-0.338395
Process [6]
-0.343428
-0.348429
-0.3534
-0.358338
-0.363244
-0.368117
-0.372957
-0.377762
-0.382533
-0.387269
Process [7]
-0.391969
-0.396634
-0.401261
-0.405851
-0.410403
-0.414917
-0.419392
-0.423828
-0.428223
-0.432578
Process [8]
-0.436892
-0.441165
-0.445396
-0.449583
-0.453728
-0.457829
-0.461886
-0.465898
-0.469866
-0.473787
Process [9]
-0.477662
-0.481491
-0.485272
-0.489005
-0.492691
-0.496327
-0.499915
-0.503453
-0.506941
0.
Norm of error 3.12204 ,The iteration is 50000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00543783
-0.0108752
-0.0163115
-0.0217463
-0.0271792
-0.0326095
-0.0380369
-0.0434607
-0.0488806
-0.0542959
Process [1]
-0.0597063
-0.0651112
-0.0705101
-0.0759025
-0.0812879
-0.0866658
-0.0920358
-0.0973973
-0.10275
-0.108093
Process [2]
-0.113426
-0.118748
-0.12406
-0.12936
-0.134648
-0.139924
-0.145187
-0.150436
-0.155671
-0.160892
Process [3]
-0.166098
-0.171289
-0.176463
-0.181621
-0.186762
-0.191886
-0.196992
-0.20208
-0.207148
-0.212198
Process [4]
-0.217227
-0.222236
-0.227224
-0.232191
-0.237136
-0.242058
-0.246958
-0.251835
-0.256688
-0.261516
Process [5]
-0.26632
-0.271099
-0.275852
-0.280578
-0.285279
-0.289951
-0.294597
-0.299214
-0.303803
-0.308363
Process [6]
-0.312893
-0.317393
-0.321863
-0.326302
-0.33071
-0.335085
-0.339429
-0.34374
-0.348017
-0.352261
Process [7]
-0.356471
-0.360647
-0.364787
-0.368892
-0.372961
-0.376994
-0.38099
-0.384949
-0.388871
-0.392754
Process [8]
-0.396599
-0.400405
-0.404172
-0.407899
-0.411586
-0.415232
-0.418838
-0.422402
-0.425925
-0.429405
Process [9]
-0.432843
-0.436238
-0.43959
-0.442898
-0.446162
-0.449382
-0.452556
-0.455686
-0.458771
0.
Norm of error 2.83801 ,The iteration is 60000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00494706
-0.00989365
-0.0148393
-0.0197835
-0.0247257
-0.0296656
-0.0346027
-0.0395363
-0.0444662
-0.0493918
Process [1]
-0.0543125
-0.0592281
-0.0641379
-0.0690415
-0.0739384
-0.0788281
-0.0837103
-0.0885843
-0.0934498
-0.0983062
Process [2]
-0.103153
-0.10799
-0.112816
-0.117632
-0.122436
-0.127228
-0.132008
-0.136775
-0.141529
-0.146269
Process [3]
-0.150995
-0.155706
-0.160402
-0.165083
-0.169747
-0.174395
-0.179026
-0.18364
-0.188236
-0.192813
Process [4]
-0.197372
-0.201911
-0.206431
-0.21093
-0.215409
-0.219867
-0.224304
-0.228718
-0.233111
-0.23748
Process [5]
-0.241826
-0.246149
-0.250447
-0.254721
-0.258971
-0.263194
-0.267392
-0.271564
-0.275709
-0.279827
Process [6]
-0.283917
-0.28798
-0.292014
-0.29602
-0.299996
-0.303943
-0.30786
-0.311746
-0.315602
-0.319427
Process [7]
-0.32322
-0.326982
-0.330711
-0.334407
-0.338071
-0.341701
-0.345297
-0.348859
-0.352386
-0.355878
Process [8]
-0.359336
-0.362757
-0.366143
-0.369492
-0.372804
-0.37608
-0.379318
-0.382518
-0.385681
-0.388805
Process [9]
-0.39189
-0.394936
-0.397943
-0.40091
-0.403837
-0.406724
-0.40957
-0.412375
-0.41514
0.
Norm of error 2.57281 ,The iteration is 70000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00448624
-0.00897204
-0.013457
-0.0179405
-0.0224224
-0.026902
-0.0313789
-0.0358528
-0.0403231
-0.0447894
Process [1]
-0.0492514
-0.0537084
-0.0581602
-0.0626062
-0.0670461
-0.0714793
-0.0759055
-0.0803242
-0.084735
-0.0891374
Process [2]
-0.093531
-0.0979154
-0.10229
-0.106655
-0.111009
-0.115352
-0.119684
-0.124004
-0.128311
-0.132606
Process [3]
-0.136888
-0.141156
-0.145411
-0.149651
-0.153876
-0.158086
-0.162281
-0.166459
-0.170621
-0.174766
Process [4]
-0.178894
-0.183004
-0.187095
-0.191169
-0.195223
-0.199258
-0.203274
-0.207269
-0.211244
-0.215198
Process [5]
-0.21913
-0.223041
-0.22693
-0.230796
-0.234639
-0.238459
-0.242256
-0.246028
-0.249776
-0.253499
Process [6]
-0.257197
-0.26087
-0.264516
-0.268136
-0.27173
-0.275297
-0.278836
-0.282347
-0.285831
-0.289286
Process [7]
-0.292712
-0.29611
-0.299477
-0.302815
-0.306123
-0.3094
-0.312647
-0.315862
-0.319046
-0.322198
Process [8]
-0.325318
-0.328406
-0.331461
-0.334483
-0.337471
-0.340426
-0.343347
-0.346234
-0.349086
-0.351903
Process [9]
-0.354685
-0.357432
-0.360143
-0.362818
-0.365456
-0.368059
-0.370624
-0.373153
-0.375644
0.
Norm of error 2.3298 ,The iteration is 80000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00406303
-0.00812566
-0.0121875
-0.0162481
-0.0203071
-0.024364
-0.0284186
-0.0324703
-0.0365188
-0.0405637
Process [1]
-0.0446045
-0.0486409
-0.0526724
-0.0566988
-0.0607194
-0.0647341
-0.0687423
-0.0727437
-0.0767378
-0.0807243
Process [2]
-0.0847028
-0.0886728
-0.092634
-0.096586
-0.100528
-0.104461
-0.108383
-0.112294
-0.116194
-0.120083
Process [3]
-0.123959
-0.127823
-0.131675
-0.135513
-0.139338
-0.143149
-0.146946
-0.150728
-0.154495
-0.158247
Process [4]
-0.161983
-0.165703
-0.169406
-0.173093
-0.176762
-0.180414
-0.184047
-0.187663
-0.19126
-0.194837
Process [5]
-0.198395
-0.201934
-0.205452
-0.20895
-0.212427
-0.215883
-0.219317
-0.22273
-0.22612
-0.229488
Process [6]
-0.232833
-0.236154
-0.239452
-0.242727
-0.245977
-0.249202
-0.252403
-0.255578
-0.258728
-0.261852
Process [7]
-0.26495
-0.268022
-0.271067
-0.274085
-0.277075
-0.280038
-0.282973
-0.285879
-0.288757
-0.291607
Process [8]
-0.294427
-0.297217
-0.299978
-0.302709
-0.30541
-0.308081
-0.31072
-0.313329
-0.315906
-0.318451
Process [9]
-0.320965
-0.323447
-0.325896
-0.328313
-0.330697
-0.333048
-0.335366
-0.33765
-0.3399
0.
Norm of error 2.10877 ,The iteration is 90000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
-0.00367776
-0.00735516
-0.0110318
-0.0147074
-0.0183815
-0.0220537
-0.0257238
-0.0293913
-0.0330559
-0.0367171
Process [1]
-0.0403747
-0.0440283
-0.0476775
-0.0513219
-0.0549612
-0.058595
-0.062223
-0.0658448
-0.06946
-0.0730682
Process [2]
-0.0766692
-0.0802625
-0.0838478
-0.0874248
-0.090993
-0.0945522
-0.0981019
-0.101642
-0.105172
-0.108691
Process [3]
-0.112199
-0.115697
-0.119182
-0.122656
-0.126118
-0.129567
-0.133003
-0.136425
-0.139834
-0.14323
Process [4]
-0.146611
-0.149977
-0.153328
-0.156664
-0.159984
-0.163289
-0.166577
-0.169848
-0.173103
-0.17634
Process [5]
-0.17956
-0.182761
-0.185945
-0.189109
-0.192255
-0.195382
-0.198489
-0.201577
-0.204644
-0.207691
Process [6]
-0.210717
-0.213722
-0.216706
-0.219668
-0.222608
-0.225526
-0.228421
-0.231294
-0.234143
-0.236969
Process [7]
-0.239772
-0.24255
-0.245304
-0.248034
-0.250739
-0.253419
-0.256073
-0.258702
-0.261305
-0.263882
Process [8]
-0.266433
-0.268957
-0.271454
-0.273924
-0.276367
-0.278782
-0.281169
-0.283528
-0.285858
-0.28816
Process [9]
-0.290434
-0.292678
-0.294893
-0.297078
-0.299234
-0.30136
-0.303456
-0.305521
-0.307556
0.
Norm of error 1.90834 ,The iteration is 100000 
Time is 13.106781 s 
Norm of error 1.90834 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./main.out on a  named r01n03 with 10 processors, by mae-cuin Thu Jun  9 12:23:39 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           1.316e+01     1.000   1.316e+01
Objects:              2.700e+01     1.000   2.700e+01
Flop:                 6.400e+06     1.103   6.020e+06  6.020e+07
Flop/sec:             4.865e+05     1.103   4.576e+05  4.576e+06
MPI Messages:         1.600e+06     3.200   7.201e+05  7.201e+06
MPI Message Lengths:  1.281e+07     3.199   8.006e+00  5.765e+07
MPI Reductions:       1.001e+05     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.3157e+01 100.0%  6.0205e+07 100.0%  7.201e+06 100.0%  8.006e+00      100.0%  1.000e+05 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided     100006 1.0 1.1611e+01 2.1 0.00e+00 0.0 1.8e+06 1.2e+01 1.0e+05 68  0 25 37100  68  0 25 37100     0
BuildTwoSidedF    100005 1.0 1.1791e+01 2.0 0.00e+00 0.0 5.4e+06 8.0e+00 1.0e+05 70  0 75 75100  70  0 75 75100     0
VecView               12 1.0 4.6077e-03 1.3 0.00e+00 0.0 2.9e+02 3.4e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               11 1.0 1.2505e-03 2.3 2.42e+02 1.1 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0     2
VecScale               1 1.0 1.9787e-02 7.4 1.10e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecCopy           100000 1.0 4.3874e-02 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 5 1.0 4.5991e-0445.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               11 1.0 1.5593e-04 1.7 2.42e+02 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    14
VecAssemblyBegin  100003 1.0 1.2075e+01 2.0 0.00e+00 0.0 5.4e+06 8.0e+00 1.0e+05 72  0 75 75100  72  0 75 75100     0
VecAssemblyEnd    100003 1.0 2.3428e+0032.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecScatterBegin   100000 1.0 2.6797e-01 2.2 0.00e+00 0.0 1.8e+06 8.0e+00 1.0e+00  2  0 25 25  0   2  0 25 25  0     0
VecScatterEnd     100000 1.0 6.2798e+00138.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 20  0  0  0  0  20  0  0  0  0     0
MatMultAdd        100000 1.0 6.6345e+0031.5 6.40e+06 1.1 1.8e+06 8.0e+00 1.0e+00 22100 25 25  0  22100 25 25  0     9
MatAssemblyBegin       3 1.0 1.1971e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         3 1.0 4.6721e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMat        1 1.0 3.5110e-03 1.1 0.00e+00 0.0 4.5e+01 9.3e+01 1.4e+01  0  0  0  0  0   0  0  0  0  0     0
MatView                1 1.0 9.6531e-03 1.1 0.00e+00 0.0 6.3e+01 6.8e+01 1.9e+01  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 2.2750e-03 3.1 0.00e+00 0.0 3.6e+01 4.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack            100000 1.0 3.5437e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack          100000 1.0 1.6441e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     8              8        14120     0.
              Viewer     3              2         1680     0.
              Matrix     7              7        37604     0.
           Index Set     7              7         6648     0.
   Star Forest Graph     2              2         2400     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.45707e-05
Average time for zero size MPI_Send(): 4.1008e-06
#PETSc Option Table entries:
-log_view
-n 100000
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/ --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/ --with-debugging=no --prefix=/work/mae-cuin/lib/petsc-3.16.6-opt --download-hypre --download-mumps --download-metis --download-hdf5 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-scalapack-include=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include --with-scalapack-lib="-L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64"
-----------------------------------------
Libraries compiled on 2022-05-04 08:04:13 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-cuin/lib/petsc-3.16.6-opt
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc  -fPIC -wd1572 -Wno-unknown-pragmas -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort  -fPIC -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-cuin/lib/petsc-3.16.6-opt/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort
Using libraries: -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -lpetsc -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

