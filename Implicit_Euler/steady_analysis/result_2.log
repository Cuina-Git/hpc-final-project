delta_t 0.001000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.00318258
1.0202
0.00953517
0.0126989
1.05127
0.0189857
0.0221025
1.08329
0.0282677
0.03131
Process [1]
1.11628
0.0372988
1.13883
0.0431405
0.0459989
1.17351
0.0515767
0.0542906
1.20925
0.0595551
Process [2]
0.0621005
1.24608
0.0670049
1.27125
0.0716449
0.07386
1.30996
0.0780693
0.0800594
1.34986
Process [3]
0.0838008
0.0855483
1.39097
0.0887884
0.0902778
1.43333
0.092988
0.0942061
1.47698
0.0963622
Process [4]
0.0972981
1.52196
0.098881
0.0995265
1.56831
0.100522
1.59999
0.101121
0.101271
1.64872
Process [5]
0.101271
0.101121
1.69893
0.100522
0.100074
1.75067
0.098881
0.098138
1.80399
0.0963622
Process [6]
1.84043
0.0942061
0.092988
1.89648
0.0902778
0.0887884
1.95424
0.0855483
1.99372
0.0819706
Process [7]
2.03399
0.0780693
0.0760021
2.09594
0.0716449
0.0693591
2.15977
0.0645846
2.2034
0.0595551
Process [8]
0.056951
2.2705
0.0515767
0.0488119
2.33965
0.0431405
0.0402395
2.4109
0.0343213
0.03131
Process [9]
2.48432
0.0251976
0.0221025
2.55998
0.0158501
0.0126989
2.63794
0.00636201
0.00318258
0.
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
3.14108e-05
6.27905e-05
9.41083e-05
0.000125333
0.000156434
0.000187381
0.000218143
0.00024869
0.000278991
0.000309017
Process [1]
0.000338738
0.000368125
0.000397148
0.000425779
0.00045399
0.000481754
0.000509041
0.000535827
0.000562083
0.000587785
Process [2]
0.000612907
0.000637424
0.000661312
0.000684547
0.000707107
0.000728969
0.000750111
0.000770513
0.000790155
0.000809017
Process [3]
0.000827081
0.000844328
0.000860742
0.000876307
0.000891007
0.000904827
0.000917755
0.000929776
0.000940881
0.000951057
Process [4]
0.000960294
0.000968583
0.000975917
0.000982287
0.000987688
0.000992115
0.000995562
0.000998027
0.000999507
0.001
Process [5]
0.000999507
0.000998027
0.000995562
0.000992115
0.000987688
0.000982287
0.000975917
0.000968583
0.000960294
0.000951057
Process [6]
0.000940881
0.000929776
0.000917755
0.000904827
0.000891007
0.000876307
0.000860742
0.000844328
0.000827081
0.000809017
Process [7]
0.000790155
0.000770513
0.000750111
0.000728969
0.000707107
0.000684547
0.000661312
0.000637424
0.000612907
0.000587785
Process [8]
0.000562083
0.000535827
0.000509041
0.000481754
0.00045399
0.000425779
0.000397148
0.000368125
0.000338738
0.000309017
Process [9]
0.000278991
0.00024869
0.000218143
0.000187381
0.000156434
0.000125333
9.41083e-05
6.27905e-05
3.14108e-05
-3.21625e-19
Mat Object: 10 MPI processes
  type: mpiaij
row 0: (0, 21.)  (1, -10.) 
row 1: (0, -10.)  (1, 21.)  (2, -10.) 
row 2: (1, -10.)  (2, 21.)  (3, -10.) 
row 3: (2, -10.)  (3, 21.)  (4, -10.) 
row 4: (3, -10.)  (4, 21.)  (5, -10.) 
row 5: (4, -10.)  (5, 21.)  (6, -10.) 
row 6: (5, -10.)  (6, 21.)  (7, -10.) 
row 7: (6, -10.)  (7, 21.)  (8, -10.) 
row 8: (7, -10.)  (8, 21.)  (9, -10.) 
row 9: (8, -10.)  (9, 21.)  (10, -10.) 
row 10: (9, -10.)  (10, 21.)  (11, -10.) 
row 11: (10, -10.)  (11, 21.)  (12, -10.) 
row 12: (11, -10.)  (12, 21.)  (13, -10.) 
row 13: (12, -10.)  (13, 21.)  (14, -10.) 
row 14: (13, -10.)  (14, 21.)  (15, -10.) 
row 15: (14, -10.)  (15, 21.)  (16, -10.) 
row 16: (15, -10.)  (16, 21.)  (17, -10.) 
row 17: (16, -10.)  (17, 21.)  (18, -10.) 
row 18: (17, -10.)  (18, 21.)  (19, -10.) 
row 19: (18, -10.)  (19, 21.)  (20, -10.) 
row 20: (19, -10.)  (20, 21.)  (21, -10.) 
row 21: (20, -10.)  (21, 21.)  (22, -10.) 
row 22: (21, -10.)  (22, 21.)  (23, -10.) 
row 23: (22, -10.)  (23, 21.)  (24, -10.) 
row 24: (23, -10.)  (24, 21.)  (25, -10.) 
row 25: (24, -10.)  (25, 21.)  (26, -10.) 
row 26: (25, -10.)  (26, 21.)  (27, -10.) 
row 27: (26, -10.)  (27, 21.)  (28, -10.) 
row 28: (27, -10.)  (28, 21.)  (29, -10.) 
row 29: (28, -10.)  (29, 21.)  (30, -10.) 
row 30: (29, -10.)  (30, 21.)  (31, -10.) 
row 31: (30, -10.)  (31, 21.)  (32, -10.) 
row 32: (31, -10.)  (32, 21.)  (33, -10.) 
row 33: (32, -10.)  (33, 21.)  (34, -10.) 
row 34: (33, -10.)  (34, 21.)  (35, -10.) 
row 35: (34, -10.)  (35, 21.)  (36, -10.) 
row 36: (35, -10.)  (36, 21.)  (37, -10.) 
row 37: (36, -10.)  (37, 21.)  (38, -10.) 
row 38: (37, -10.)  (38, 21.)  (39, -10.) 
row 39: (38, -10.)  (39, 21.)  (40, -10.) 
row 40: (39, -10.)  (40, 21.)  (41, -10.) 
row 41: (40, -10.)  (41, 21.)  (42, -10.) 
row 42: (41, -10.)  (42, 21.)  (43, -10.) 
row 43: (42, -10.)  (43, 21.)  (44, -10.) 
row 44: (43, -10.)  (44, 21.)  (45, -10.) 
row 45: (44, -10.)  (45, 21.)  (46, -10.) 
row 46: (45, -10.)  (46, 21.)  (47, -10.) 
row 47: (46, -10.)  (47, 21.)  (48, -10.) 
row 48: (47, -10.)  (48, 21.)  (49, -10.) 
row 49: (48, -10.)  (49, 21.)  (50, -10.) 
row 50: (49, -10.)  (50, 21.)  (51, -10.) 
row 51: (50, -10.)  (51, 21.)  (52, -10.) 
row 52: (51, -10.)  (52, 21.)  (53, -10.) 
row 53: (52, -10.)  (53, 21.)  (54, -10.) 
row 54: (53, -10.)  (54, 21.)  (55, -10.) 
row 55: (54, -10.)  (55, 21.)  (56, -10.) 
row 56: (55, -10.)  (56, 21.)  (57, -10.) 
row 57: (56, -10.)  (57, 21.)  (58, -10.) 
row 58: (57, -10.)  (58, 21.)  (59, -10.) 
row 59: (58, -10.)  (59, 21.)  (60, -10.) 
row 60: (59, -10.)  (60, 21.)  (61, -10.) 
row 61: (60, -10.)  (61, 21.)  (62, -10.) 
row 62: (61, -10.)  (62, 21.)  (63, -10.) 
row 63: (62, -10.)  (63, 21.)  (64, -10.) 
row 64: (63, -10.)  (64, 21.)  (65, -10.) 
row 65: (64, -10.)  (65, 21.)  (66, -10.) 
row 66: (65, -10.)  (66, 21.)  (67, -10.) 
row 67: (66, -10.)  (67, 21.)  (68, -10.) 
row 68: (67, -10.)  (68, 21.)  (69, -10.) 
row 69: (68, -10.)  (69, 21.)  (70, -10.) 
row 70: (69, -10.)  (70, 21.)  (71, -10.) 
row 71: (70, -10.)  (71, 21.)  (72, -10.) 
row 72: (71, -10.)  (72, 21.)  (73, -10.) 
row 73: (72, -10.)  (73, 21.)  (74, -10.) 
row 74: (73, -10.)  (74, 21.)  (75, -10.) 
row 75: (74, -10.)  (75, 21.)  (76, -10.) 
row 76: (75, -10.)  (76, 21.)  (77, -10.) 
row 77: (76, -10.)  (77, 21.)  (78, -10.) 
row 78: (77, -10.)  (78, 21.)  (79, -10.) 
row 79: (78, -10.)  (79, 21.)  (80, -10.) 
row 80: (79, -10.)  (80, 21.)  (81, -10.) 
row 81: (80, -10.)  (81, 21.)  (82, -10.) 
row 82: (81, -10.)  (82, 21.)  (83, -10.) 
row 83: (82, -10.)  (83, 21.)  (84, -10.) 
row 84: (83, -10.)  (84, 21.)  (85, -10.) 
row 85: (84, -10.)  (85, 21.)  (86, -10.) 
row 86: (85, -10.)  (86, 21.)  (87, -10.) 
row 87: (86, -10.)  (87, 21.)  (88, -10.) 
row 88: (87, -10.)  (88, 21.)  (89, -10.) 
row 89: (88, -10.)  (89, 21.)  (90, -10.) 
row 90: (89, -10.)  (90, 21.)  (91, -10.) 
row 91: (90, -10.)  (91, 21.)  (92, -10.) 
row 92: (91, -10.)  (92, 21.)  (93, -10.) 
row 93: (92, -10.)  (93, 21.)  (94, -10.) 
row 94: (93, -10.)  (94, 21.)  (95, -10.) 
row 95: (94, -10.)  (95, 21.)  (96, -10.) 
row 96: (95, -10.)  (96, 21.)  (97, -10.) 
row 97: (96, -10.)  (97, 21.)  (98, -10.) 
row 98: (97, -10.)  (98, 21.)  (99, -10.) 
row 99: (98, -10.)  (99, 21.)  (100, -10.) 
row 100: (99, -10.)  (100, 21.) 
Error is 0.000743 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

../main.out on a  named r01n07 with 10 processors, by mae-cuin Thu Jun  9 21:02:28 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           1.030e+00     1.000   1.030e+00
Objects:              7.300e+01     1.000   7.300e+01
Flop:                 2.493e+06     1.102   2.329e+06  2.329e+07
Flop/sec:             2.420e+06     1.102   2.261e+06  2.261e+07
MPI Messages:         3.314e+04     1.101   3.254e+04  3.254e+05
MPI Message Lengths:  2.674e+05     1.100   8.077e+00  2.628e+06
MPI Reductions:       1.609e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0287e+00  99.8%  2.3293e+07 100.0%  3.254e+05 100.0%  8.077e+00      100.0%  1.608e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided       1009 1.0 8.5987e-02 3.1 0.00e+00 0.0 1.8e+04 1.2e+01 1.0e+03  6  0  6  8  6   6  0  6  8  6     0
BuildTwoSidedF      1005 1.0 6.9738e-02 1.8 0.00e+00 0.0 5.5e+04 8.4e+00 1.0e+03  5  0 17 17  6   5  0 17 17  6     0
VecView                2 1.0 1.0520e-0246.9 0.00e+00 0.0 1.8e+01 8.4e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             7015 1.0 2.5426e-01 2.3 5.91e+05 1.1 0.0e+00 0.0e+00 7.0e+03 17 23  0  0 44  17 23  0  0 44    21
VecNorm             8016 1.0 4.0787e-01 1.2 1.76e+05 1.1 0.0e+00 0.0e+00 8.0e+03 37  7  0  0 50  37  7  0  0 50     4
VecScale            8016 1.0 9.3811e-03 2.0 8.82e+04 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0    86
VecCopy             2000 1.0 4.7421e-04 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet             26054 1.0 4.4270e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             1001 1.0 5.4884e-04 3.0 2.20e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   368
VecAYPX             1000 1.0 5.0378e-04 2.4 2.20e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   401
VecMAXPY            8015 1.0 2.8865e-03 2.2 7.73e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0 30  0  0  0   0 30  0  0  0  2459
VecAssemblyBegin    1003 1.0 6.8435e-02 1.7 0.00e+00 0.0 5.5e+04 8.4e+00 1.0e+03  5  0 17 17  6   5  0 17 17  6     0
VecAssemblyEnd      1003 1.0 2.5139e-0216.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecScatterBegin    39075 1.0 6.6614e-02 1.6 0.00e+00 0.0 2.7e+05 8.0e+00 2.0e+00  5  0 83 82  0   5  0 83 82  0     0
VecScatterEnd      39075 1.0 1.6817e-01 8.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 11  0  0  0  0  11  0  0  0  0     0
VecNormalize        8015 1.0 4.1410e-01 1.2 2.64e+05 1.1 0.0e+00 0.0e+00 8.0e+03 37 10  0  0 50  37 10  0  0 50     6
MatMult             7015 1.0 1.3870e-01 3.9 3.72e+05 1.1 1.3e+05 8.0e+00 1.0e+00  7 15 39 38  0   7 15 39 38  0    25
MatSolve            8015 1.0 2.4054e-03 1.7 4.49e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0 19  0  0  0   0 19  0  0  0  1849
MatLUFactorNum         1 1.0 6.8188e-0511.0 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     7
MatILUFactorSym        1 1.0 5.8889e-05 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 4.6852e-0344.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         4 1.0 1.9074e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+00  2  0  0  0  0   2  0  0  0  0     0
MatGetRowIJ            1 1.0 3.9816e-0513.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 7.0720e-03 1.0 0.00e+00 0.0 9.0e+01 1.4e+01 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatCreateSubMat        1 1.0 4.0028e-02 1.2 0.00e+00 0.0 4.5e+01 9.3e+01 1.4e+01  4  0  0  0  0   4  0  0  0  0     0
MatGetOrdering         1 1.0 7.7009e-05 4.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.3931e-02100.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatView                1 1.0 6.6839e-02 1.3 0.00e+00 0.0 6.3e+01 6.8e+01 1.9e+01  5  0  0  0  0   5  0  0  0  0     0
SFSetGraph             5 1.0 3.3379e-06 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 2.1201e-02 2.8 0.00e+00 0.0 7.2e+01 4.0e+00 2.0e+00  2  0  0  0  0   2  0  0  0  0     0
SFReduceBegin       8015 1.0 2.5420e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd         8015 1.0 2.3453e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack             39075 1.0 6.6891e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack           39075 1.0 4.2834e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 9.8944e-05 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve            1000 1.0 8.0342e-01 1.1 2.47e+06 1.1 2.7e+05 8.0e+00 1.5e+04 76 99 83 82 93  77 99 83 82 94    29
KSPGMRESOrthog      7015 1.0 2.5786e-01 2.2 1.21e+06 1.1 0.0e+00 0.0e+00 7.0e+03 18 48  0  0 44  18 48  0  0 44    43
PCSetUp                2 1.0 4.5668e-02 1.3 4.50e+01 1.1 9.0e+01 1.4e+01 7.0e+00  4  0  0  0  0   4  0  0  0  0     0
PCSetUpOnBlocks     1000 1.0 1.0462e-03 4.3 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply             8015 1.0 2.0702e-01 3.1 4.49e+05 1.1 1.4e+05 8.0e+00 1.0e+00 13 19 44 44  0  13 19 44 44  0    21
PCApplyOnBlocks     8015 1.0 2.2288e-02 2.1 4.49e+05 1.1 0.0e+00 0.0e+00 0.0e+00  1 19  0  0  0   1 19  0  0  0   200
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    29             29        52192     0.
              Viewer     3              2         1680     0.
              Matrix    10             10        48680     0.
           Index Set    16             16        14976     0.
   IS L to G Mapping     1              1          768     0.
   Star Forest Graph     7              7         8112     0.
       Krylov Solver     2              2        20440     0.
      Preconditioner     2              2         2056     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          896     0.
           Weak Form     1              1          616     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.7643e-05
Average time for zero size MPI_Send(): 5.4121e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-n 1000
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/ --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/ --with-debugging=no --prefix=/work/mae-cuin/lib/petsc-3.16.6-opt --download-hypre --download-mumps --download-metis --download-hdf5 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-scalapack-include=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include --with-scalapack-lib="-L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64"
-----------------------------------------
Libraries compiled on 2022-05-04 08:04:13 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-cuin/lib/petsc-3.16.6-opt
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc  -fPIC -wd1572 -Wno-unknown-pragmas -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort  -fPIC -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-cuin/lib/petsc-3.16.6-opt/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort
Using libraries: -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -lpetsc -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

