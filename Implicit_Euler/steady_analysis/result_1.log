delta_t 0.010000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
1.01005
1.0202
1.03045
1.04081
1.05127
1.06184
1.07251
1.08329
1.09417
1.10517
Process [1]
1.11628
1.1275
1.13883
1.15027
1.16183
1.17351
1.1853
1.19722
1.20925
1.2214
Process [2]
1.23368
1.24608
1.2586
1.27125
1.28403
1.29693
1.30996
1.32313
1.33643
1.34986
Process [3]
1.36343
1.37713
1.39097
1.40495
1.41907
1.43333
1.44773
1.46228
1.47698
1.49182
Process [4]
1.50682
1.52196
1.53726
1.55271
1.56831
1.58407
1.59999
1.61607
1.63232
1.64872
Process [5]
1.66529
1.68203
1.69893
1.71601
1.73325
1.75067
1.76827
1.78604
1.80399
1.82212
Process [6]
1.84043
1.85893
1.87761
1.89648
1.91554
1.93479
1.95424
1.97388
1.99372
2.01375
Process [7]
2.03399
2.05443
2.07508
2.09594
2.117
2.13828
2.15977
2.18147
2.2034
2.22554
Process [8]
2.24791
2.2705
2.29332
2.31637
2.33965
2.36316
2.38691
2.4109
2.43513
2.4596
Process [9]
2.48432
2.50929
2.53451
2.55998
2.58571
2.6117
2.63794
2.66446
2.69123
0.
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.000314108
0.000627905
0.000941083
0.00125333
0.00156434
0.00187381
0.00218143
0.0024869
0.00278991
0.00309017
Process [1]
0.00338738
0.00368125
0.00397148
0.00425779
0.0045399
0.00481754
0.00509041
0.00535827
0.00562083
0.00587785
Process [2]
0.00612907
0.00637424
0.00661312
0.00684547
0.00707107
0.00728969
0.00750111
0.00770513
0.00790155
0.00809017
Process [3]
0.00827081
0.00844328
0.00860742
0.00876307
0.00891007
0.00904827
0.00917755
0.00929776
0.00940881
0.00951057
Process [4]
0.00960294
0.00968583
0.00975917
0.00982287
0.00987688
0.00992115
0.00995562
0.00998027
0.00999507
0.01
Process [5]
0.00999507
0.00998027
0.00995562
0.00992115
0.00987688
0.00982287
0.00975917
0.00968583
0.00960294
0.00951057
Process [6]
0.00940881
0.00929776
0.00917755
0.00904827
0.00891007
0.00876307
0.00860742
0.00844328
0.00827081
0.00809017
Process [7]
0.00790155
0.00770513
0.00750111
0.00728969
0.00707107
0.00684547
0.00661312
0.00637424
0.00612907
0.00587785
Process [8]
0.00562083
0.00535827
0.00509041
0.00481754
0.0045399
0.00425779
0.00397148
0.00368125
0.00338738
0.00309017
Process [9]
0.00278991
0.0024869
0.00218143
0.00187381
0.00156434
0.00125333
0.000941083
0.000627905
0.000314108
-3.21625e-18
Mat Object: 10 MPI processes
  type: mpiaij
row 0: (0, 201.)  (1, -100.) 
row 1: (0, -100.)  (1, 201.)  (2, -100.) 
row 2: (1, -100.)  (2, 201.)  (3, -100.) 
row 3: (2, -100.)  (3, 201.)  (4, -100.) 
row 4: (3, -100.)  (4, 201.)  (5, -100.) 
row 5: (4, -100.)  (5, 201.)  (6, -100.) 
row 6: (5, -100.)  (6, 201.)  (7, -100.) 
row 7: (6, -100.)  (7, 201.)  (8, -100.) 
row 8: (7, -100.)  (8, 201.)  (9, -100.) 
row 9: (8, -100.)  (9, 201.)  (10, -100.) 
row 10: (9, -100.)  (10, 201.)  (11, -100.) 
row 11: (10, -100.)  (11, 201.)  (12, -100.) 
row 12: (11, -100.)  (12, 201.)  (13, -100.) 
row 13: (12, -100.)  (13, 201.)  (14, -100.) 
row 14: (13, -100.)  (14, 201.)  (15, -100.) 
row 15: (14, -100.)  (15, 201.)  (16, -100.) 
row 16: (15, -100.)  (16, 201.)  (17, -100.) 
row 17: (16, -100.)  (17, 201.)  (18, -100.) 
row 18: (17, -100.)  (18, 201.)  (19, -100.) 
row 19: (18, -100.)  (19, 201.)  (20, -100.) 
row 20: (19, -100.)  (20, 201.)  (21, -100.) 
row 21: (20, -100.)  (21, 201.)  (22, -100.) 
row 22: (21, -100.)  (22, 201.)  (23, -100.) 
row 23: (22, -100.)  (23, 201.)  (24, -100.) 
row 24: (23, -100.)  (24, 201.)  (25, -100.) 
row 25: (24, -100.)  (25, 201.)  (26, -100.) 
row 26: (25, -100.)  (26, 201.)  (27, -100.) 
row 27: (26, -100.)  (27, 201.)  (28, -100.) 
row 28: (27, -100.)  (28, 201.)  (29, -100.) 
row 29: (28, -100.)  (29, 201.)  (30, -100.) 
row 30: (29, -100.)  (30, 201.)  (31, -100.) 
row 31: (30, -100.)  (31, 201.)  (32, -100.) 
row 32: (31, -100.)  (32, 201.)  (33, -100.) 
row 33: (32, -100.)  (33, 201.)  (34, -100.) 
row 34: (33, -100.)  (34, 201.)  (35, -100.) 
row 35: (34, -100.)  (35, 201.)  (36, -100.) 
row 36: (35, -100.)  (36, 201.)  (37, -100.) 
row 37: (36, -100.)  (37, 201.)  (38, -100.) 
row 38: (37, -100.)  (38, 201.)  (39, -100.) 
row 39: (38, -100.)  (39, 201.)  (40, -100.) 
row 40: (39, -100.)  (40, 201.)  (41, -100.) 
row 41: (40, -100.)  (41, 201.)  (42, -100.) 
row 42: (41, -100.)  (42, 201.)  (43, -100.) 
row 43: (42, -100.)  (43, 201.)  (44, -100.) 
row 44: (43, -100.)  (44, 201.)  (45, -100.) 
row 45: (44, -100.)  (45, 201.)  (46, -100.) 
row 46: (45, -100.)  (46, 201.)  (47, -100.) 
row 47: (46, -100.)  (47, 201.)  (48, -100.) 
row 48: (47, -100.)  (48, 201.)  (49, -100.) 
row 49: (48, -100.)  (49, 201.)  (50, -100.) 
row 50: (49, -100.)  (50, 201.)  (51, -100.) 
row 51: (50, -100.)  (51, 201.)  (52, -100.) 
row 52: (51, -100.)  (52, 201.)  (53, -100.) 
row 53: (52, -100.)  (53, 201.)  (54, -100.) 
row 54: (53, -100.)  (54, 201.)  (55, -100.) 
row 55: (54, -100.)  (55, 201.)  (56, -100.) 
row 56: (55, -100.)  (56, 201.)  (57, -100.) 
row 57: (56, -100.)  (57, 201.)  (58, -100.) 
row 58: (57, -100.)  (58, 201.)  (59, -100.) 
row 59: (58, -100.)  (59, 201.)  (60, -100.) 
row 60: (59, -100.)  (60, 201.)  (61, -100.) 
row 61: (60, -100.)  (61, 201.)  (62, -100.) 
row 62: (61, -100.)  (62, 201.)  (63, -100.) 
row 63: (62, -100.)  (63, 201.)  (64, -100.) 
row 64: (63, -100.)  (64, 201.)  (65, -100.) 
row 65: (64, -100.)  (65, 201.)  (66, -100.) 
row 66: (65, -100.)  (66, 201.)  (67, -100.) 
row 67: (66, -100.)  (67, 201.)  (68, -100.) 
row 68: (67, -100.)  (68, 201.)  (69, -100.) 
row 69: (68, -100.)  (69, 201.)  (70, -100.) 
row 70: (69, -100.)  (70, 201.)  (71, -100.) 
row 71: (70, -100.)  (71, 201.)  (72, -100.) 
row 72: (71, -100.)  (72, 201.)  (73, -100.) 
row 73: (72, -100.)  (73, 201.)  (74, -100.) 
row 74: (73, -100.)  (74, 201.)  (75, -100.) 
row 75: (74, -100.)  (75, 201.)  (76, -100.) 
row 76: (75, -100.)  (76, 201.)  (77, -100.) 
row 77: (76, -100.)  (77, 201.)  (78, -100.) 
row 78: (77, -100.)  (78, 201.)  (79, -100.) 
row 79: (78, -100.)  (79, 201.)  (80, -100.) 
row 80: (79, -100.)  (80, 201.)  (81, -100.) 
row 81: (80, -100.)  (81, 201.)  (82, -100.) 
row 82: (81, -100.)  (82, 201.)  (83, -100.) 
row 83: (82, -100.)  (83, 201.)  (84, -100.) 
row 84: (83, -100.)  (84, 201.)  (85, -100.) 
row 85: (84, -100.)  (85, 201.)  (86, -100.) 
row 86: (85, -100.)  (86, 201.)  (87, -100.) 
row 87: (86, -100.)  (87, 201.)  (88, -100.) 
row 88: (87, -100.)  (88, 201.)  (89, -100.) 
row 89: (88, -100.)  (89, 201.)  (90, -100.) 
row 90: (89, -100.)  (90, 201.)  (91, -100.) 
row 91: (90, -100.)  (91, 201.)  (92, -100.) 
row 92: (91, -100.)  (92, 201.)  (93, -100.) 
row 93: (92, -100.)  (93, 201.)  (94, -100.) 
row 94: (93, -100.)  (94, 201.)  (95, -100.) 
row 95: (94, -100.)  (95, 201.)  (96, -100.) 
row 96: (95, -100.)  (96, 201.)  (97, -100.) 
row 97: (96, -100.)  (97, 201.)  (98, -100.) 
row 98: (97, -100.)  (98, 201.)  (99, -100.) 
row 99: (98, -100.)  (99, 201.)  (100, -100.) 
row 100: (99, -100.)  (100, 201.) 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.00634863
0.00953558
0.0127162
0.0158874
0.0190461
0.0221891
0.0253132
0.0284155
0.0314929
0.0345423
Process [1]
0.0375608
0.0405452
0.0434928
0.0464006
0.0492657
0.0520853
0.0548566
0.0575768
0.0602434
0.0628537
Process [2]
0.065405
0.0678949
0.0703209
0.0726806
0.0749717
0.0771919
0.0793391
0.0814111
0.0834059
0.0853214
Process [3]
0.0871559
0.0889075
0.0905745
0.0921552
0.093648
0.0950516
0.0963645
0.0975854
0.0987131
0.0997465
Process [4]
0.100685
0.101526
0.102271
0.102918
0.103467
0.103916
0.104266
0.104517
0.104667
0.104717
Process [5]
0.104667
0.104517
0.104266
0.103916
0.103467
0.102918
0.102271
0.101526
0.100685
0.0997465
Process [6]
0.0987131
0.0975854
0.0963645
0.0950516
0.093648
0.0921552
0.0905745
0.0889075
0.0871559
0.0853214
Process [7]
0.0834059
0.0814111
0.0793391
0.0771919
0.0749717
0.0726806
0.0703209
0.0678949
0.065405
0.0628536
Process [8]
0.0602434
0.0575768
0.0548566
0.0520853
0.0492657
0.0464006
0.0434928
0.0405452
0.0375608
0.0345423
Process [9]
0.0314929
0.0284155
0.0253132
0.0221891
0.0190461
0.0158874
0.0127162
0.00953558
0.00634863
0.
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

../main.out on a  named r01n15 with 10 processors, by mae-cuin Thu Jun  9 05:03:18 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           8.425e-01     1.000   8.424e-01
Objects:              8.200e+01     1.000   8.200e+01
Flop:                 8.693e+05     1.102   8.063e+05  8.063e+06
Flop/sec:             1.032e+06     1.102   9.571e+05  9.571e+06
MPI Messages:         7.011e+03     1.435   6.592e+03  6.592e+04
MPI Message Lengths:  5.839e+04     1.412   8.389e+00  5.529e+05
MPI Reductions:       3.477e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 8.4176e-01  99.9%  8.0630e+06 100.0%  6.592e+04 100.0%  8.389e+00      100.0%  3.459e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided        108 1.0 7.2900e-02 1.4 0.00e+00 0.0 2.0e+03 1.2e+01 1.1e+02  7  0  3  4  3   7  0  3  4  3     0
BuildTwoSidedF       104 1.0 5.9772e-02 2.2 0.00e+00 0.0 5.9e+03 1.1e+01 1.0e+02  6  0  9 12  3   6  0  9 12  3     0
VecView                3 1.0 1.2642e-0225.0 0.00e+00 0.0 4.5e+01 5.2e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1608 1.0 1.8925e-01 1.9 2.89e+05 1.1 0.0e+00 0.0e+00 1.6e+03 18 33  0  0 46  18 33  0  0 46    14
VecNorm             1708 1.0 1.8446e-01 1.7 3.76e+04 1.1 0.0e+00 0.0e+00 1.7e+03 19  4  0  0 49  19  4  0  0 49     2
VecScale            1709 1.0 9.8009e-03 2.6 1.88e+04 1.1 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0    18
VecCopy              200 1.0 5.7697e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              5332 1.0 1.1523e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              100 1.0 6.1750e-05 3.0 2.20e+03 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   327
VecAYPX              100 1.0 2.0361e-04 2.4 2.20e+03 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    99
VecMAXPY            1708 1.0 9.3627e-04 2.3 3.38e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0 39  0  0  0   0 39  0  0  0  3318
VecAssemblyBegin     102 1.0 5.0662e-02 1.9 0.00e+00 0.0 5.9e+03 1.1e+01 1.0e+02  5  0  9 12  3   5  0  9 12  3     0
VecAssemblyEnd       102 1.0 8.5015e-0334.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecScatterBegin     8440 1.0 3.7506e-02 1.8 0.00e+00 0.0 6.0e+04 8.0e+00 2.0e+00  3  0 91 86  0   3  0 91 86  0     0
VecScatterEnd       8440 1.0 1.0715e-0123.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   6  0  0  0  0     0
VecNormalize        1708 1.0 1.8521e-01 1.7 5.64e+04 1.1 0.0e+00 0.0e+00 1.7e+03 19  6  0  0 49  19  6  0  0 49     3
MatMult             1608 1.0 1.0648e-01 8.9 8.52e+04 1.1 2.9e+04 8.0e+00 1.0e+00  4 10 44 42  0   4 10 44 42  0     8
MatSolve            1708 1.0 5.5552e-04 1.9 9.56e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0 12  0  0  0   0 12  0  0  0  1706
MatLUFactorNum         1 1.0 4.4203e-0488.3 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     1
MatILUFactorSym        1 1.0 3.3593e-0415.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 1.0091e-02103.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyEnd         4 1.0 5.2972e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+00  5  0  0  0  0   5  0  0  0  0     0
MatGetRowIJ            1 1.0 2.2411e-04104.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 1.2200e-0264.6 0.00e+00 0.0 9.0e+01 1.4e+01 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatCreateSubMat        1 1.0 8.1991e-02 1.0 0.00e+00 0.0 4.5e+01 9.3e+01 1.4e+01  9  0  0  1  0   9  0  0  1  0     0
MatGetOrdering         1 1.0 2.7204e-0418.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 1.9997e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatView                1 1.0 1.3924e-01 1.2 0.00e+00 0.0 6.3e+01 6.8e+01 1.9e+01 16  0  0  1  1  16  0  0  1  1     0
SFSetGraph             5 1.0 2.8610e-06 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 2.5508e-02 1.9 0.00e+00 0.0 7.2e+01 4.0e+00 2.0e+00  2  0  0  0  0   2  0  0  0  0     0
SFReduceBegin       1708 1.0 5.5194e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd         1708 1.0 4.7469e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              8440 1.0 1.5800e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            8440 1.0 9.8896e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 4.5419e-0410.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             100 1.0 4.0939e-01 1.1 8.67e+05 1.1 6.0e+04 8.0e+00 3.3e+03 47100 91 86 95  47100 91 86 96    20
KSPGMRESOrthog      1608 1.0 1.9054e-01 1.9 5.92e+05 1.1 0.0e+00 0.0e+00 1.6e+03 18 67  0  0 46  18 67  0  0 46    28
PCSetUp                2 1.0 1.3424e-01 6.2 4.50e+01 1.1 9.0e+01 1.4e+01 7.0e+00 13  0  0  0  0  13  0  0  0  0     0
PCSetUpOnBlocks      100 1.0 8.7500e-04 7.4 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     1
PCApply             1708 1.0 1.0517e-01 5.3 9.56e+04 1.1 3.1e+04 8.0e+00 1.0e+00  5 12 47 45  0   5 12 47 45  0     9
PCApplyOnBlocks     1708 1.0 6.1450e-03 2.7 9.56e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0 12  0  0  0   0 12  0  0  0   154
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    38             38        68752     0.
              Viewer     3              2         1680     0.
              Matrix    10             10        48680     0.
           Index Set    16             16        14976     0.
   IS L to G Mapping     1              1          768     0.
   Star Forest Graph     7              7         8112     0.
       Krylov Solver     2              2        20440     0.
      Preconditioner     2              2         2056     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          896     0.
           Weak Form     1              1          616     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.57628e-05
Average time for zero size MPI_Send(): 4.1008e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-n 100
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/ --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/ --with-debugging=no --prefix=/work/mae-cuin/lib/petsc-3.16.6-opt --download-hypre --download-mumps --download-metis --download-hdf5 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-scalapack-include=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include --with-scalapack-lib="-L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64"
-----------------------------------------
Libraries compiled on 2022-05-04 08:04:13 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-cuin/lib/petsc-3.16.6-opt
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc  -fPIC -wd1572 -Wno-unknown-pragmas -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort  -fPIC -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-cuin/lib/petsc-3.16.6-opt/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort
Using libraries: -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -lpetsc -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

