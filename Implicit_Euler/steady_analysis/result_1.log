delta_t 0.010000 
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.00318258
1.0202
0.00953517
0.0126989
1.05127
0.0189857
0.0221025
1.08329
0.0282677
0.03131
Process [1]
1.11628
0.0372988
1.13883
0.0431405
0.0459989
1.17351
0.0515767
0.0542906
1.20925
0.0595551
Process [2]
0.0621005
1.24608
0.0670049
1.27125
0.0716449
0.07386
1.30996
0.0780693
0.0800594
1.34986
Process [3]
0.0838008
0.0855483
1.39097
0.0887884
0.0902778
1.43333
0.092988
0.0942061
1.47698
0.0963622
Process [4]
0.0972981
1.52196
0.098881
0.0995265
1.56831
0.100522
1.59999
0.101121
0.101271
1.64872
Process [5]
0.101271
0.101121
1.69893
0.100522
0.100074
1.75067
0.098881
1.78604
0.0972981
0.0963622
Process [6]
1.84043
0.0942061
0.092988
1.89648
0.0902778
0.0887884
1.95424
0.0855483
1.99372
0.0819706
Process [7]
2.03399
0.0780693
0.0760021
2.09594
0.0716449
0.0693591
2.15977
0.0645846
2.2034
0.0595551
Process [8]
0.056951
2.2705
0.0515767
0.0488119
2.33965
0.0431405
0.0402395
2.4109
0.0343213
0.03131
Process [9]
2.48432
0.0251976
0.0221025
2.55998
0.0158501
0.0126989
2.63794
0.00636201
0.00318258
0.
Vec Object: 10 MPI processes
  type: mpi
Process [0]
0.
0.000314108
0.000627905
0.000941083
0.00125333
0.00156434
0.00187381
0.00218143
0.0024869
0.00278991
0.00309017
Process [1]
0.00338738
0.00368125
0.00397148
0.00425779
0.0045399
0.00481754
0.00509041
0.00535827
0.00562083
0.00587785
Process [2]
0.00612907
0.00637424
0.00661312
0.00684547
0.00707107
0.00728969
0.00750111
0.00770513
0.00790155
0.00809017
Process [3]
0.00827081
0.00844328
0.00860742
0.00876307
0.00891007
0.00904827
0.00917755
0.00929776
0.00940881
0.00951057
Process [4]
0.00960294
0.00968583
0.00975917
0.00982287
0.00987688
0.00992115
0.00995562
0.00998027
0.00999507
0.01
Process [5]
0.00999507
0.00998027
0.00995562
0.00992115
0.00987688
0.00982287
0.00975917
0.00968583
0.00960294
0.00951057
Process [6]
0.00940881
0.00929776
0.00917755
0.00904827
0.00891007
0.00876307
0.00860742
0.00844328
0.00827081
0.00809017
Process [7]
0.00790155
0.00770513
0.00750111
0.00728969
0.00707107
0.00684547
0.00661312
0.00637424
0.00612907
0.00587785
Process [8]
0.00562083
0.00535827
0.00509041
0.00481754
0.0045399
0.00425779
0.00397148
0.00368125
0.00338738
0.00309017
Process [9]
0.00278991
0.0024869
0.00218143
0.00187381
0.00156434
0.00125333
0.000941083
0.000627905
0.000314108
-3.21625e-18
Mat Object: 10 MPI processes
  type: mpiaij
row 0: (0, 201.)  (1, -100.) 
row 1: (0, -100.)  (1, 201.)  (2, -100.) 
row 2: (1, -100.)  (2, 201.)  (3, -100.) 
row 3: (2, -100.)  (3, 201.)  (4, -100.) 
row 4: (3, -100.)  (4, 201.)  (5, -100.) 
row 5: (4, -100.)  (5, 201.)  (6, -100.) 
row 6: (5, -100.)  (6, 201.)  (7, -100.) 
row 7: (6, -100.)  (7, 201.)  (8, -100.) 
row 8: (7, -100.)  (8, 201.)  (9, -100.) 
row 9: (8, -100.)  (9, 201.)  (10, -100.) 
row 10: (9, -100.)  (10, 201.)  (11, -100.) 
row 11: (10, -100.)  (11, 201.)  (12, -100.) 
row 12: (11, -100.)  (12, 201.)  (13, -100.) 
row 13: (12, -100.)  (13, 201.)  (14, -100.) 
row 14: (13, -100.)  (14, 201.)  (15, -100.) 
row 15: (14, -100.)  (15, 201.)  (16, -100.) 
row 16: (15, -100.)  (16, 201.)  (17, -100.) 
row 17: (16, -100.)  (17, 201.)  (18, -100.) 
row 18: (17, -100.)  (18, 201.)  (19, -100.) 
row 19: (18, -100.)  (19, 201.)  (20, -100.) 
row 20: (19, -100.)  (20, 201.)  (21, -100.) 
row 21: (20, -100.)  (21, 201.)  (22, -100.) 
row 22: (21, -100.)  (22, 201.)  (23, -100.) 
row 23: (22, -100.)  (23, 201.)  (24, -100.) 
row 24: (23, -100.)  (24, 201.)  (25, -100.) 
row 25: (24, -100.)  (25, 201.)  (26, -100.) 
row 26: (25, -100.)  (26, 201.)  (27, -100.) 
row 27: (26, -100.)  (27, 201.)  (28, -100.) 
row 28: (27, -100.)  (28, 201.)  (29, -100.) 
row 29: (28, -100.)  (29, 201.)  (30, -100.) 
row 30: (29, -100.)  (30, 201.)  (31, -100.) 
row 31: (30, -100.)  (31, 201.)  (32, -100.) 
row 32: (31, -100.)  (32, 201.)  (33, -100.) 
row 33: (32, -100.)  (33, 201.)  (34, -100.) 
row 34: (33, -100.)  (34, 201.)  (35, -100.) 
row 35: (34, -100.)  (35, 201.)  (36, -100.) 
row 36: (35, -100.)  (36, 201.)  (37, -100.) 
row 37: (36, -100.)  (37, 201.)  (38, -100.) 
row 38: (37, -100.)  (38, 201.)  (39, -100.) 
row 39: (38, -100.)  (39, 201.)  (40, -100.) 
row 40: (39, -100.)  (40, 201.)  (41, -100.) 
row 41: (40, -100.)  (41, 201.)  (42, -100.) 
row 42: (41, -100.)  (42, 201.)  (43, -100.) 
row 43: (42, -100.)  (43, 201.)  (44, -100.) 
row 44: (43, -100.)  (44, 201.)  (45, -100.) 
row 45: (44, -100.)  (45, 201.)  (46, -100.) 
row 46: (45, -100.)  (46, 201.)  (47, -100.) 
row 47: (46, -100.)  (47, 201.)  (48, -100.) 
row 48: (47, -100.)  (48, 201.)  (49, -100.) 
row 49: (48, -100.)  (49, 201.)  (50, -100.) 
row 50: (49, -100.)  (50, 201.)  (51, -100.) 
row 51: (50, -100.)  (51, 201.)  (52, -100.) 
row 52: (51, -100.)  (52, 201.)  (53, -100.) 
row 53: (52, -100.)  (53, 201.)  (54, -100.) 
row 54: (53, -100.)  (54, 201.)  (55, -100.) 
row 55: (54, -100.)  (55, 201.)  (56, -100.) 
row 56: (55, -100.)  (56, 201.)  (57, -100.) 
row 57: (56, -100.)  (57, 201.)  (58, -100.) 
row 58: (57, -100.)  (58, 201.)  (59, -100.) 
row 59: (58, -100.)  (59, 201.)  (60, -100.) 
row 60: (59, -100.)  (60, 201.)  (61, -100.) 
row 61: (60, -100.)  (61, 201.)  (62, -100.) 
row 62: (61, -100.)  (62, 201.)  (63, -100.) 
row 63: (62, -100.)  (63, 201.)  (64, -100.) 
row 64: (63, -100.)  (64, 201.)  (65, -100.) 
row 65: (64, -100.)  (65, 201.)  (66, -100.) 
row 66: (65, -100.)  (66, 201.)  (67, -100.) 
row 67: (66, -100.)  (67, 201.)  (68, -100.) 
row 68: (67, -100.)  (68, 201.)  (69, -100.) 
row 69: (68, -100.)  (69, 201.)  (70, -100.) 
row 70: (69, -100.)  (70, 201.)  (71, -100.) 
row 71: (70, -100.)  (71, 201.)  (72, -100.) 
row 72: (71, -100.)  (72, 201.)  (73, -100.) 
row 73: (72, -100.)  (73, 201.)  (74, -100.) 
row 74: (73, -100.)  (74, 201.)  (75, -100.) 
row 75: (74, -100.)  (75, 201.)  (76, -100.) 
row 76: (75, -100.)  (76, 201.)  (77, -100.) 
row 77: (76, -100.)  (77, 201.)  (78, -100.) 
row 78: (77, -100.)  (78, 201.)  (79, -100.) 
row 79: (78, -100.)  (79, 201.)  (80, -100.) 
row 80: (79, -100.)  (80, 201.)  (81, -100.) 
row 81: (80, -100.)  (81, 201.)  (82, -100.) 
row 82: (81, -100.)  (82, 201.)  (83, -100.) 
row 83: (82, -100.)  (83, 201.)  (84, -100.) 
row 84: (83, -100.)  (84, 201.)  (85, -100.) 
row 85: (84, -100.)  (85, 201.)  (86, -100.) 
row 86: (85, -100.)  (86, 201.)  (87, -100.) 
row 87: (86, -100.)  (87, 201.)  (88, -100.) 
row 88: (87, -100.)  (88, 201.)  (89, -100.) 
row 89: (88, -100.)  (89, 201.)  (90, -100.) 
row 90: (89, -100.)  (90, 201.)  (91, -100.) 
row 91: (90, -100.)  (91, 201.)  (92, -100.) 
row 92: (91, -100.)  (92, 201.)  (93, -100.) 
row 93: (92, -100.)  (93, 201.)  (94, -100.) 
row 94: (93, -100.)  (94, 201.)  (95, -100.) 
row 95: (94, -100.)  (95, 201.)  (96, -100.) 
row 96: (95, -100.)  (96, 201.)  (97, -100.) 
row 97: (96, -100.)  (97, 201.)  (98, -100.) 
row 98: (97, -100.)  (98, 201.)  (99, -100.) 
row 99: (98, -100.)  (99, 201.)  (100, -100.) 
row 100: (99, -100.)  (100, 201.) 
Error is 0.007456 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

../main.out on a  named r01n07 with 10 processors, by mae-cuin Thu Jun  9 21:02:26 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           6.825e-01     1.000   6.824e-01
Objects:              8.300e+01     1.000   8.300e+01
Flop:                 8.557e+05     1.102   7.938e+05  7.938e+06
Flop/sec:             1.254e+06     1.102   1.163e+06  1.163e+07
MPI Messages:         6.952e+03     1.428   6.540e+03  6.540e+04
MPI Message Lengths:  5.789e+04     1.405   8.382e+00  5.482e+05
MPI Reductions:       3.449e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.8235e-01 100.0%  7.9379e+06 100.0%  6.540e+04 100.0%  8.382e+00      100.0%  3.431e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided        109 1.0 6.4992e-02 1.9 0.00e+00 0.0 2.0e+03 1.2e+01 1.1e+02  8  0  3  4  3   8  0  3  4  3     0
BuildTwoSidedF       105 1.0 5.2212e-02 1.5 0.00e+00 0.0 6.0e+03 1.1e+01 1.0e+02  6  0  9 12  3   6  0  9 12  3     0
VecView                2 1.0 1.6110e-0269.4 0.00e+00 0.0 1.8e+01 8.4e+01 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecMDot             1593 1.0 8.4309e-02 3.3 2.84e+05 1.1 0.0e+00 0.0e+00 1.6e+03  8 33  0  0 46   8 33  0  0 46    31
VecNorm             1694 1.0 1.0774e-01 1.7 3.73e+04 1.1 0.0e+00 0.0e+00 1.7e+03 13  4  0  0 49  13  4  0  0 49     3
VecScale            1694 1.0 1.7035e-0150.4 1.86e+04 1.1 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0     1
VecCopy              200 1.0 7.7724e-05 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              5288 1.0 1.1590e-03 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              101 1.0 6.5565e-05 3.4 2.22e+03 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   311
VecAYPX              100 1.0 3.5085e-02448.7 2.20e+03 1.1 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     1
VecMAXPY            1693 1.0 8.9788e-04 2.3 3.32e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0 38  0  0  0   0 38  0  0  0  3398
VecAssemblyBegin     103 1.0 4.5349e-02 1.4 0.00e+00 0.0 6.0e+03 1.1e+01 1.0e+02  5  0  9 12  3   5  0  9 12  3     0
VecAssemblyEnd       103 1.0 2.2028e-0257.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecScatterBegin     8365 1.0 2.4195e-02 1.8 0.00e+00 0.0 5.9e+04 8.0e+00 2.0e+00  3  0 91 86  0   3  0 91 86  0     0
VecScatterEnd       8365 1.0 5.1975e-0211.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
VecNormalize        1693 1.0 1.0841e-01 1.7 5.59e+04 1.1 0.0e+00 0.0e+00 1.7e+03 13  6  0  0 49  13  6  0  0 49     5
MatMult             1593 1.0 4.3303e-02 3.3 8.44e+04 1.1 2.9e+04 8.0e+00 1.0e+00  3 10 44 42  0   3 10 44 42  0    18
MatSolve            1693 1.0 5.5313e-04 1.8 9.48e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0 12  0  0  0   0 12  0  0  0  1699
MatLUFactorNum         1 1.0 3.3140e-05 5.6 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    13
MatILUFactorSym        1 1.0 5.9843e-05 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 8.0152e-0370.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyEnd         4 1.0 3.7226e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+00  5  0  0  0  0   5  0  0  0  0     0
MatGetRowIJ            1 1.0 2.0981e-05 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMats       1 1.0 7.9222e-03 1.6 0.00e+00 0.0 9.0e+01 1.4e+01 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatCreateSubMat        1 1.0 6.9997e-02 1.2 0.00e+00 0.0 4.5e+01 9.3e+01 1.4e+01 10  0  0  1  0  10  0  0  1  0     0
MatGetOrdering         1 1.0 5.1975e-05 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatIncreaseOvrlp       1 1.0 9.2368e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatView                1 1.0 1.1683e-01 1.2 0.00e+00 0.0 6.3e+01 6.8e+01 1.9e+01 15  0  0  1  1  15  0  0  1  1     0
SFSetGraph             5 1.0 3.8147e-06 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                4 1.0 1.4497e-0260.6 0.00e+00 0.0 7.2e+01 4.0e+00 2.0e+00  2  0  0  0  0   2  0  0  0  0     0
SFReduceBegin       1693 1.0 5.0211e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd         1693 1.0 5.1332e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack              8365 1.0 1.4603e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            8365 1.0 1.0412e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 2.3794e-04 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             100 1.0 2.1178e-01 1.0 8.53e+05 1.1 5.9e+04 8.0e+00 3.3e+03 31100 91 86 95  31100 91 86 96    37
KSPGMRESOrthog      1593 1.0 8.6510e-02 2.2 5.81e+05 1.1 0.0e+00 0.0e+00 1.6e+03  9 67  0  0 46   9 67  0  0 46    62
PCSetUp                2 1.0 6.4399e-02 2.8 4.50e+01 1.1 9.0e+01 1.4e+01 7.0e+00  8  0  0  0  0   8  0  0  0  0     0
PCSetUpOnBlocks      100 1.0 2.6894e-04 1.8 4.50e+01 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
PCApply             1693 1.0 6.6824e-02 3.2 9.48e+04 1.1 3.1e+04 8.0e+00 1.0e+00  5 12 47 44  0   5 12 47 44  0    14
PCApplyOnBlocks     1693 1.0 5.5943e-03 2.6 9.48e+04 1.1 0.0e+00 0.0e+00 0.0e+00  1 12  0  0  0   1 12  0  0  0   168
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    39             39        70592     0.
              Viewer     3              2         1680     0.
              Matrix    10             10        48680     0.
           Index Set    16             16        14976     0.
   IS L to G Mapping     1              1          768     0.
   Star Forest Graph     7              7         8112     0.
       Krylov Solver     2              2        20440     0.
      Preconditioner     2              2         2056     0.
    Distributed Mesh     1              1         5048     0.
     Discrete System     1              1          896     0.
           Weak Form     1              1          616     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.92165e-05
Average time for zero size MPI_Send(): 4.81606e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-n 100
-pc_type asm
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/ --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/ --with-debugging=no --prefix=/work/mae-cuin/lib/petsc-3.16.6-opt --download-hypre --download-mumps --download-metis --download-hdf5 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-scalapack-include=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/include --with-scalapack-lib="-L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64"
-----------------------------------------
Libraries compiled on 2022-05-04 08:04:13 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-cuin/lib/petsc-3.16.6-opt
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc  -fPIC -wd1572 -Wno-unknown-pragmas -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort  -fPIC -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-cuin/lib/petsc-3.16.6-opt/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort
Using libraries: -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -lpetsc -Wl,-rpath,/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/work/mae-cuin/lib/petsc-3.16.6-opt/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lmkl_blacs_intelmpi_lp64 -lmkl_scalapack_lp64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

